---
title: Modality for HAR
date: 2024-08-26 21:34:23
tags: ["deep learning", "action recognition"]
---

> 论文：Human Action Recognition From Various Data Modalities: A Review

---
*这篇综述侧重讲解了动作检测中不同的建模方式。*

## 不同的建模方法

提到了10种建模方式，优缺点已列出
![不同建模](images/HAR2/modalities.png)

---
## RGB

使用RGB摄像机获取，大部分相关工作使用视频，少部分工作使用静止的图片。

在深度学习早期，很多方法是手工的。基于时间，空间，体积的的方法，基于空间上的兴趣点的方法，基于运动轨迹的方法等，都适用与RGB建模。现在的主流是设计网络。
现在的网络可以分为三种：
1. 双流二维卷积神经网络
2. 循环神经网络
3. 三维卷积网络
   三种网络的整理如下：
   ![整理](images/HAR2/frames.png)
## 双流网络

![双流网络](images/HAR2/two_stream.png)
双流网络有两个二维卷积网络分支，从RGB视频例提前两种不同特征后输入两个分支，最终结果通过fusion两条分支的结果得到。

在经典的双流网络中，Simonyan与Zisserman使用了空间的网络和时间的网络，分别输入两个流；另一个经典的设计由Karpathy完成，两个流分别接收视频中央部分
裁切下来的视频，一个分辨率低，一个分辨率高，这样能提高计算的速度。

Wang等人将不同大小的帧和光流输入两个流，由此提取卷积特征图，然后以提取的轨迹为中心进行采样，用Fisher Vector聚合，再输入到SVM里。

其他工作：
Wang: 将视频分成3段，每段分别使用双流网络处理，检测的评分用平均池化融合，得到对视频的预测；

Diba: 用逐元素乘法将没一节视频的特征

Girdhar: 基于双流网络采样表现和动作的帧，提取特征，然后使用描述动作的词语的词典来将特征聚合成视频级的表征，用来检测动作。

Feichtenhofer: 将特征层面的动作信息与外观残差特征相乘，用来实现门控调制。

Zong: 通过增加动作特征流将双流网络拓展成三流网络，实现更好地捕捉动作特征信息

Bilen: 使用rank pooling，用RGB流和光流构建动态图，从而归纳全局的表观和动作。动态图，RGB图和光流图共同输入多流网络中，用来实现动作检测。

## 光流法的问题

计算光流的开销很大，为解决这个问题，Zhang等人提出一种teacher-student framework，将用光流信息训练的导师网络迁移到用动作向量训练的学生网络，动作向
量可以通过压缩的视频获得，不需要额外的计算。

Piergiovanni等人提出一种可以训练的flow layer，它不需要计算光流就可以捕捉动作特征。

## 在其他方面拓展双流网络的工作

Wang等人提出一种更深的双流CNN；Kar等人递归地预测每一帧对于动作检测的重要性；Zhang等人为了在低分辨率的视频上实现动作检测，提出了两种方法生成高分辨率
的视频；一些工作证明在最后一个卷积层融合空间和时间网络能够在减少网络参数的同时提高准确率。

## RNN

![RNN](images/HAR2/explainrnn.png)基于RNN的方法一般使用二维卷积网络来提取特征，再用一个lstm来检测动作，Donahue等人提出了LRCN，这种网络用
2d cnn提取帧级的rgb信息，然后用lstm实现对动作的标记。吴恩达等人从预训练的二维卷积神经网络提取帧级别的rgb和图片流的特征，然后把这些特征输入到lstm堆栈，实现动作检测；编码lstm也可以用来将输入的视频映射到固定长度的representation里，然后用解码lstm解码，完成无监督的视频重建和预测。Wu等人使用两个lstm，分别操作与稀疏和精细的的cnn特征，共同实现高效的动作检测。Majd和Safabakhsh提出一种$C^2$的LSTM，使用卷积和互相关计算来学习前向和后向的时间信息。

## 注意力

Sharma：多层lstm递归产生attention map，关注重要的空间信息，实现更好的检测效果；

Sudhakaran：带有内建空间注意力的递归单元，在空间上定位视频的偏差信息

Li：提出Video-LSTM，将卷积和基于动作的注意力合并入soft-attention LSTM，从而更好地捕捉动作和空间信息。

## GRUs

能更好解决纯粹的RNN的梯度消失问题，较LSTM有更少的门，更少的模型参数，但是在动作检测方面表现和LSTM差不多

## 混合结构

组合2D CNN和RNN。例如Wu利用双流2D CNN提取空间和短期动作特征，然后分别输入到LSTM里完成对时序信息的建模。

## 基于三维卷积的方法

Tran: 3D CNN模型，即C3D，在端到端网络中从视频里学习时空特征，主要用于片段层面的学习

Diba: 使用3D卷积核和pooling kernals来拓展DenseNet，并设计了一个时序三维卷积网络(T3D)，在这个网络中，时间过渡层可以对多种时序卷积核的深度进行
建模，T3D可以稠密且有效地捕捉多种长度视频的表观和时序信息。之后他们又将一些网络块嵌入一些结构，比如ResNext和ResNet，从而实现在时空特征方面对3维卷积
核的内部通道之间的联系的建模。

Varol: 提出一种长期时序卷积(LTC)网络，以减少空间分辨率为代价延长三维卷积网络表达时间信息的极限，完成对长时间的时序结构的建模。

Hussein: 提出多种大小，只关注时序的卷积，又称为时间感知(Timeception)，用来应对复杂而持续时间长的动作的多样性和时间长短的不同。

Wang: 提出一种非本地的操作，在特征图中对任意两个位置的关系之间进行建模，从而捕捉长范围的依赖。

Li: 提出通道独立的方向卷积(CIDC)，可以被加在I3D之后以更好地捕捉完整视频的长期时序动态。

## 3D CNN外加双流或多流

Carreira与Zisserman: two-stream inflated 3d cnn(I3D)，用具有额外时序维的二维卷积核膨胀卷积和池化核

Wang: two-stream cnn 与lstm融合，捕捉长距离时序依赖

Feichtenhofer: 包含slow pathway和fast pathway的two-stream 3d cnn，对慢速和高速的rgb帧分别捕获语义和动作

Li: 双流时空可变形3d cnn，外加注意力，可以捕获长范围的时序和长距离的空间依赖

## 对3D CNNs问题的解决


